# -*- coding: utf-8 -*-
"""MainModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pb1iayF-96Lp2xVehvPT2nOg_Hqy906k

##Modelo Principal / Main Model
"""

import math
import numpy as np
import pandas as pd
from collections import Counter
import csv
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
import os

from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
import imblearn
from imblearn.over_sampling import SMOTE

from keras.models import Sequential
from keras.layers import Dense

# CALCULO DE LOS BIGOTES DE UNA LISTA DE VALORES

def bigotes(xx):
    q25 = np.quantile(xx, 0.25)
    q75 = np.quantile(xx, 0.75)
    RIC = q75 - q25
    BI = q25 - 1.5*RIC
    BS = q75 + 1.5*RIC

    minimo = xx.min()
    maximo = xx.max()

    if BI < minimo:
        BI = minimo

    if BS > maximo:
        BS = maximo

    return (BI, BS)

# BUSCA LAS COLUMNAS QUE TIENEN UN PORCENTAJE DE VACIOS MAYOR O IGUAL A UN VALOR

def columnas_vacias(dff, porcentaje):
    L = []
    b = dff.shape[0]

    for col in dff.columns:
        a = sum(dff[col].isnull())
        r = np.round(100*a/b, 2)
        if r > porcentaje:
            L.append(col)

    return L

# columnas_vacias(40)
# columnas_vacias(df_nueva, 10)

# PREPARA Google Drive
drive.mount('/gdrive')

# LECTURA DEL DATASET
ruta = '/gdrive/MyDrive/UNAL_Cobros.csv'
df = pd.read_csv(ruta, sep=";")

nbytes = os.stat(ruta).st_size
megabytes = nbytes / (1024 ** 2)
print(f"El archivo tiene {megabytes:.2f} MB")

print(type(df))

# INFORMACION DE LAS VARIABLES DEL DATASET
df.info()

# EJEMPLO DE LOS DATOS DEL DATASET
df.head()

# TAMAÑO DEL DATASET
print("Numero de filas: ", df.shape[0])
print("Numero de variables: ", df.shape[1])

# ELIMINACION DE COLUMNAS CON INFORMACION NO RELEVANTE
df2 = df.drop(['FECHA_ENVIO', 'CUST_CODE', 'NUMERO_FACTURA',
       'FECHA_DEVOLUCION', 'FECHA_FACTURA',
       'FECHA CARGA', 'EMAIL', 'IMPORTE PRODUCTO'], axis=1)

# COLUMNAS RESULTANTES DESPUES DE LA ELIMINACION
df2.columns

# VARIABLE TARGET, CORRESPONDE A COBRANZA SI/NO
df2['TARGET']

# CAMBIAR LA VARIABLE TARGET DE TIPO CATEGORICO A NUMERICO BINARIO

df2['TARGET'] = [1 if x=='SI' else 0 for x in df2['TARGET']]

df2['TARGET'][0:5]

df2

df2.info()

#OBTENEMOS LOS VALORES ESTADISTICOS BASICOS DE CADA UNA DE LAS VARIABLES
df2.describe()

df2["TARGET"].value_counts().plot(kind='barh', width=0.7, edgecolor='black')

df2["ESTADO_CLIENTE"].value_counts().plot(kind='pie', autopct='%.2f%%', wedgeprops={"linewidth": 2, "edgecolor": "white"})

sns.histplot(data=df2, x="CANTIDAD_FACTURAS", bins=10, kde=True)

# ANALISIS DE VARIABLE PERMANENCIA
Counter(df2.PERMANENCIA)

"""Como la variable PERMANENCIA tiene tantos valores vacios, la variable no sera usada."""

# BORRADO DE LA VARIABLE PERMANENCIA
df2.drop("PERMANENCIA", axis=1, inplace=True)

# ANALISIS DE VARIABLE FEU
Counter(df2.FEU)

"""Como la variable FEU tiene tantos valores vacios, la variable no sera usada."""

# BORRADO DE LA VARIABLE FEU
df2.drop("FEU", axis=1, inplace=True)

# ANALISIS DE VARIABLE TIPO_DOCUMENTO
Counter(df2.TIPO_DOCUMENTO)

"""Como la variable TIPO_DOCUMENTO tiene tantos valores vacios, la variable no sera usada."""

# BORRADO DE LA VARIABLE TIPO_DOCUMENTO
df2.drop("TIPO_DOCUMENTO", axis=1, inplace=True)

# ANALISIS DE VARIABLE FECHA_BPI
df2.FECHA_BPI

"""En la variable FECHA_BPI los valores diferentes a NaN indican que hay fecha de baja, los valores NaN indica que no.

Por tanto crearemos una nueva variable numerica llamada BAJA así:
<br><br>
Si FECHA_BPI = NaN => BAJA=0<br>
Sino => BAJA=1
"""

# CREAMOS LA VARIABLE BAJA
df2["BAJA"] = [0 if math.isnan(x) else 1 for x in df2['FECHA_BPI']]

df2[["FECHA_BPI","BAJA"]]

# ELIMINAMOS LA VARIABLE FECHA_BPI
df2.drop("FECHA_BPI", axis=1, inplace=True)

# ANALISIS DE LA VARIABLE PORTADO
Counter(df2.PORTADO)

# AJUSTE DE LOS DATOS DE PORTADO
df2['PORTADO'] = df2['PORTADO'].map({"NO" : 0,
                                  "SI" : 1,
                                  "N" : 0,
                                  "BAJAS_I " : 0,
                                  "BAJAS_II " : 0,
                                  "N/A " : 0})

# PORTADO DESPUES DE LA TRANSFORMACION
Counter(df2.PORTADO)

# ANALISIS DE LA VARIABLE ESCENARIO RECOBROS
Counter(df2.ESCENARIO_RECOBRO)

# ANALISIS DE LA VARIABLE ESCENARIO RECOBROS
def escenario(x):
  if x == "PER" or x == "FU":
    return 1
  else:
    return 0

df2["ESCENARIO_RECOBRO"]	= [escenario(x) for x in df2['ESCENARIO_RECOBRO']]

# ESCENARIO RECOBRO DESPUES DEL CAMBIO
Counter(df2.ESCENARIO_RECOBRO)

# ANALISIS DE LA VARIABLE ESTADO_CLIENTE
Counter(df2.ESTADO_CLIENTE)

"""Como la variable ESTADO_CLIENTE tiene solo dos clases usaremos el valor 1 para la clase A y el 0 para la clase B."""

# CAMBIAR LA VARIABLE ESTADO_CLIENTE DE TIPO CATEGORICO A NUMERICO BINARIO
df2["ESTADO_CLIENTE"]	= [1 if x == "A" else 0 for x in df2['ESTADO_CLIENTE']]

# TRANFORMACION DE LA VARIABLE VAP_SCF CON VALORES SI Y NO A 1 Y 0
df2["VAP_SCF"]	= [0 if x == "N" else 1 for x in df2["VAP_SCF"]]

# TRANFORMACION DE LA VARIABLE VAP_OB CON VALORES SI Y NO A 1 Y 0
df2["VAP_OB"]	= [0 if x == "N" else 1 for x in df2["VAP_OB"]]

# TRANFORMACION DE LA VARIABLE VPT CON VALORES SI Y NO A 1 Y 0
df2["VPT"]	= [0 if x == "N" else 1 for x in df2["VPT"]]

df2

# TRANSFORMACION DEL CODIGO_POSTAL TOMANDO SOLO EL CODIGO DE LA PROVINCIA
def cp(x):
  if x != "":
    return float(x)//1000
  else:
    return x

df2["CODIGO_POSTAL"] = [cp(x) for x in df2["CODIGO_POSTAL"]]

# CODIGO POSTAL TRANSFORMADO
df2["CODIGO_POSTAL"]

# ANALISIS DE LA VARIABLE EDAD
Counter(df2.EDAD)

# IMPUTACION DEL CODIGO_POSTAL
# VAMOS A USAR LA MODA DE ESTA VARIABLE POR SER UNA PROVINCIA

df2.groupby(["CODIGO_POSTAL"])["CODIGO_POSTAL"].count().sort_values(ascending=False).head(10)

# EL VALOR DE LA MODA ES EL CODIGO 28

# VAMOS A REMPLAZAR LOS VACIOS EN LAS PROVINCIA CON LA MODA, EL VALOR ES 28
df2["CODIGO_POSTAL"].fillna(value=28, inplace=True)

df2

df2.info()

# CAMBIO IMPORTE_FACTURA DE CATEGORICO A NUMERICO
df2["IMPORTE_FACTURA"] = df2["IMPORTE_FACTURA"].astype("float")

# IMPUTACION DE VALORES A LAS VARIABLES VACIAS

cols20 = columnas_vacias(df2, 20)
for columna in cols20:
    # print("10: ", columna)
    if df2[columna].dtype == "object":
        df2[columna].fillna(value="NO IDENTIFICADO", inplace=True)
        print(columna, " imputada con NO IDENTIFICADO por tener mas del 20% de vacios")
    else:
        mediana = df2[columna].median(skipna=True)  # numeric_onlybool
        df2[columna].fillna(value=mediana, inplace=True)
        print(columna, " imputada con la mediana por tener mas del 20% de vacios: ", mediana)


cols0 = columnas_vacias(df2, 0)
for columna in cols0:
    # print("0: ", columna)
    if df2[columna].dtype == "object":
        moda = df2[columna].mode()[0]
        df2[columna].fillna(value=moda, inplace=True)
        print(columna, " imputada con la moda por tener entre 0% y 20% de vacios: ", moda)
    else:
        mediana = df[columna].median(skipna=True)  # numeric_onlybool
        df2[columna].fillna(value=mediana, inplace=True)
        print(columna, " imputada con la mediana por tener entre 0% y 20% de vacios: ", mediana)

df2

# CALCULO DE LOS BIGOTES PARA VARIABLES

lista_numericas = ["CANTIDAD_FACTURAS", "IMPORTE_FACTURA", "DEUDA_TOTAL", "NUM_LINEAS_ACTIVAS"]

for columna in lista_numericas:
    BI, BS = bigotes(df2[columna])
    print("{}: {:,.2f}, {:,.2f}".format(columna, BI, BS))

# AJUSTE DE CANTIDAD_FACTURAS SEGUN EL BIGOTE
df2['CANTIDAD_FACTURAS'] = [1   if x < 1   else x for x in df2['CANTIDAD_FACTURAS']]
df2['CANTIDAD_FACTURAS'] = [3.5 if x > 3.5 else x for x in df2['CANTIDAD_FACTURAS']]

# AJUSTE DE IMPORTE_FACTURA SEGUN EL BIGOTE
df2['IMPORTE_FACTURA'] = [0      if x < 0      else x for x in df2['IMPORTE_FACTURA']]
df2['IMPORTE_FACTURA'] = [314.24 if x > 314.24 else x for x in df2['IMPORTE_FACTURA']]

# AJUSTE DE IMPORTE_FACTURA SEGUN EL BIGOTE
df2['DEUDA_TOTAL'] = [0      if x < 0      else x for x in df2['DEUDA_TOTAL']]
df2['DEUDA_TOTAL'] = [628.83 if x > 628.83 else x for x in df2['DEUDA_TOTAL']]

# AJUSTE DE IMPORTE_FACTURA SEGUN EL BIGOTE
df2['NUM_LINEAS_ACTIVAS'] = [0   if x < 0   else x for x in df2['NUM_LINEAS_ACTIVAS']]
df2['NUM_LINEAS_ACTIVAS'] = [2.5 if x > 2.5 else x for x in df2['NUM_LINEAS_ACTIVAS']]

df2["EDAD"]

# CREACION DE VARIABLES DUMMIES PARA LA EDAD
modelo_dummy = OneHotEncoder(sparse=False)
df2_dummy = modelo_dummy.fit_transform(pd.DataFrame(df2["EDAD"]))
df2_dummy = pd.DataFrame(df2_dummy, columns=["0_30", "31_50", "51_65", "MAYOR_65"])
df2_dummy

# CONCATENA DATASET NUMERICO CON EL DATASET DE DUMMIES DE LA EDAD
df3 = pd.concat([df2, df2_dummy], axis=1)
df3.sample(20)

# ELIMINA LA VARIABLE EDAD
df3.drop("EDAD", axis=1, inplace=True)

# DISTRIBUCIÓN DE LOS DATOS (HISTOGRAMAS)
fig = plt.figure(figsize = (15,8))
axes = fig.gca()
df3[["CANTIDAD_FACTURAS", "IMPORTE_FACTURA", "DEUDA_TOTAL", "NUM_LINEAS_ACTIVAS"]].hist(ax = axes);

# GRAFICA DE CORRELACION LINEAL ENTRE LAS VARIABLES NUMERICAS

sns.set_theme(style="ticks")

# df = sns.load_dataset("penguins")

lista_numericas = ["CANTIDAD_FACTURAS", "IMPORTE_FACTURA", "DEUDA_TOTAL", "NUM_LINEAS_ACTIVAS", 'TARGET']

sns.pairplot(df3[lista_numericas], hue='TARGET')

sns.heatmap(df2.corr(), annot=True, cmap="icefire")

# SEPARACION DE X Y
df3_X = df3.drop("TARGET", axis=1)
df3_y = df3["TARGET"]

# ESCALADO DE VARIABLES NUMERICAS
modelo_sc = MinMaxScaler()

df3_X_escalado = modelo_sc.fit_transform(df3_X)

df4_X = pd.DataFrame(df3_X_escalado, columns=df3_X.columns)

df4_X.sample(5)

X_ent, X_pru, y_ent, y_pru = train_test_split(df4_X,
                                              df3_y,
                                              test_size=0.2,
                                              random_state=1)

print(X_ent.shape, X_pru.shape, y_ent.shape, y_pru.shape)

# OBSERVACIONES POR CATEFORIA EN LA VARIABLE TARGET
sns.countplot(x='TARGET', data=pd.DataFrame(y_ent))

plt.title("Datos desbalanceados");

# DATOS TOTALES SIN BALANCEAR
print(Counter(y_ent))

"""Como podemos ver la cantidad de observaciones de las dos categorías es bastante diferente, si hacemos el modelo con estas cantidades posiblemente obtendremos buena Exactitud ya que el modelo clasifica bien los registros de forma global, pero observando detenidamente nos daremos cuenta que el modelo está clasificando de buena forma la clase mayoritaria mientras que la clase minoritaria sería clasificada pobremente.
Para evitar este problema debemos usar un método que balancee las clases o que aplique una penalización sobre las mismas. Para nuestro caso balanceamos la variable usando el método SMOTE (Synthetic Minority Over-sample Tecnique) que nos permitirá crear registros sintéticos para la clase minoritaria.
"""

# BALANCE DE VARIABLE TARGET (OVERSAMPLING)
oversample = SMOTE()      # SMOTE(sampling_strategy=0.1)
X_ent_bal, y_ent_bal = oversample.fit_resample(X_ent, y_ent)

# OBSERVACIONES POR CATEFORIA EN LA VARIABLE TARGET DESPUES DEL BALANCE
sns.countplot(x='TARGET', data=pd.DataFrame(y_ent_bal))

plt.title("Datos balanceados");

# DATOS TOTALES SIN BALANCEAR
print(Counter(y_ent_bal))

# EJEMPLO DE LOS DATOS
X_ent_bal.sample(10)

X_ent_bal.info()

y_ent_bal.info()

y_pru.info()

#NUMERO DE VARIABLES
nvars = X_ent_bal.shape[1]

# CREACION DEL MODELO Y ADICION DE CAPAS
model_RMSprop = Sequential()
model_RMSprop.add(Dense(8, input_dim=nvars))
model_RMSprop.add(Dense(4, activation='relu'))
model_RMSprop.add(Dense(1, activation='sigmoid'))

# COMPILACION DEL MODELO CON OPTIMIZADOR RMSprop, VALIDACION CRUZADA Y METRICA ACCURACY
model_RMSprop.compile(optimizer='RMSprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# RESUMEN DEL MODELO
model_RMSprop.summary()

#
history = model_RMSprop.fit(X_ent_bal,
                            y_ent_bal,
                            epochs=40,
                            validation_data=(X_pru, y_pru),
                            batch_size=32)

from plotly.subplots import make_subplots
import plotly.graph_objects as go

def plot_all(history):
  fig1 = go.Figure()
  fig1.add_trace(go.Scattergl(y=history.history['accuracy'], name='Train'))
  fig1.add_trace(go.Scattergl(y=history.history['val_accuracy'], name='Valid'))
  fig1.update_layout(height=300, width=400,xaxis_title='Epoch',yaxis_title='accuracy')

  fig2 = go.Figure()
  fig2.add_trace(go.Scattergl(y=history.history['loss'], name='Train'))
  fig2.add_trace(go.Scattergl(y=history.history['val_loss'], name='Valid'))
  fig2.update_layout(height=300, width=400,xaxis_title='Epoch', yaxis_title='Loss')

  return [fig1,fig2]

fig1,fig2=plot_all(history)
fig1.show()
fig2.show()

"""## **Entrenamiento del Modelo y Selección de Hiperparámetros**

"""

!pip install -q -U keras-tuner
import keras_tuner
from keras.optimizers import Adam

def model_builder(hp):

  model = Sequential()

  hp_units_1 = hp.Int('dense_1_units', min_value=9, max_value=27, step=3)     #  default = 12

  hp_units_2 = hp.Int('dense_2_units', min_value=32, max_value=512, step=32)

  hp_units_3 = hp.Int('dense_3_units', min_value=32, max_value=512, step=32)

  model.add(Dense(units=hp_units_1, activation='relu'))

  model.add(Dense(units=hp_units_2, activation='relu'))

  model.add(Dense(units=hp_units_3, activation='relu'))

  model.add(Dense(1, activation='sigmoid'))

  hyperparameter_learning_rate = hp.Choice('learning_rate', values=(1e-2, 1e-3, 1e-4, 1e-5))

  model.compile(optimizer=Adam(learning_rate=hyperparameter_learning_rate),
                loss='binary_crossentropy',
                metrics=['accuracy'])

  return model

tuner = keras_tuner.RandomSearch(
                     model_builder,             # Función para construir hipermodelos
                     objective='val_accuracy',  # Métrica que buscamos optimizar
                     directory='runs',          # Directorio donde guardaremos los logs y resultados de los experimentos
                     project_name='keras_tuner',
                     overwrite=True)

tuner.search_space_summary()

tuner.search(X_ent_bal, y_ent_bal, epochs=50, validation_split=.2, verbose=True)

# OBTENEMOS EL MEJOR MODELO
best_model = tuner.get_best_models()[0]

best_model

# OBTENEMOS LOS MEJORES HIPERPARAMETROS ARROJADOS POR LA BUSQUEDA
best_hyperparameters = tuner.get_best_hyperparameters()[0]

best_hyperparameters.get("learning_rate")

# CONSTRUIMOS UN MODELO CON LOS MEJORES HIPERPARAMETROS
model = tuner.hypermodel.build(best_hyperparameters)

# ENTRENAMOS EL MODELO CON LOS MEJORES HIPERPARAMETROS POR 50 EPOCHS

history = model.fit(X_ent_bal,
                            y_ent_bal,
                            epochs=50,
                            validation_split=.2,
                            batch_size=32)

# EXTRAIGAMOS EL NUMERO OPTIMO DE ITERACIONES A ENTRENAR EL MODELO
best_epoch = history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1
print(f'Número óptimo de epochs: {best_epoch}')

# ENTRENEMOS UN MODELO CON LOS MEJORES HIPERPARAMETROS Y EL MEJOR NUMERO DE EPOCHS
best_model = tuner.hypermodel.build(best_hyperparameters)
best_model.fit(X_ent_bal,
                y_ent_bal,
                epochs=best_epoch,
                validation_split=.2,
                batch_size=32)

"""## **Evaluación o Aplicación del modelo**"""

# EVALUEMOS EL MODELO SOBRE LOS DATOS DE PRUEBAS
result = best_model.evaluate(X_pru, y_pru, verbose=0)
print(f'Pérdida de prueba: {result[0]}')
print(f'Exactitud de prueba: {result[1] * 100:.2f}%')

best_model.summary()